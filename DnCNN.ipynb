{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                              \n",
    "import torch.nn as nn                      \n",
    "import torch.optim as optim                \n",
    "from torch.utils.data import DataLoader    \n",
    "import torchvision                       \n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np                         \n",
    "import cv2                                \n",
    "from skimage.metrics import structural_similarity as ssim  \n",
    "import matplotlib.pyplot as plt           \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(denoised, ground_truth):\n",
    "    mse = np.mean((denoised - ground_truth) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    PIXEL_MAX = 1.0 \n",
    "    psnr = 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "    return psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def calculate_ssim(denoised, ground_truth):\n",
    "    return ssim(ground_truth, denoised, data_range=ground_truth.max() - ground_truth.min(), win_size=7, channel_axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DnCNN (Denoising Convolutional Neural Network) is a deep learning model used for image denoising.\n",
    "    It learns to predict the noise residual and subtract it from the input image to recover the clean image.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=3, num_of_layers=17, features=64):\n",
    "        super(DnCNN, self).__init__()\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=3, padding=1, bias=True))\n",
    "        layers.append(nn.ReLU(inplace=True)) \n",
    "\n",
    "        for _ in range(num_of_layers - 2):\n",
    "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False))  \n",
    "            layers.append(nn.BatchNorm2d(features))  \n",
    "            layers.append(nn.ReLU(inplace=True))  \n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=3, padding=1, bias=False)) \n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass takes an input tensor (x), applies the DnCNN network, and subtracts the predicted noise\n",
    "        from the input to output the denoised image.\n",
    "        \"\"\"\n",
    "        noise = self.dncnn(x) \n",
    "        return x - noise  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DnCNN(channels=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 20\n",
    "noise_std = 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/20], Loss: 0.010775, Time: 73.54 sec\n",
      "Epoch [2/20], Loss: 0.004080, Time: 70.58 sec\n",
      "Epoch [3/20], Loss: 0.002379, Time: 70.63 sec\n",
      "Epoch [4/20], Loss: 0.002127, Time: 68.28 sec\n",
      "Epoch [5/20], Loss: 0.001918, Time: 67.57 sec\n",
      "Epoch [6/20], Loss: 0.001874, Time: 67.63 sec\n",
      "Epoch [7/20], Loss: 0.001813, Time: 67.40 sec\n",
      "Epoch [8/20], Loss: 0.001724, Time: 67.56 sec\n",
      "Epoch [9/20], Loss: 0.001658, Time: 67.47 sec\n",
      "Epoch [10/20], Loss: 0.001636, Time: 67.52 sec\n",
      "Epoch [11/20], Loss: 0.001787, Time: 69.58 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [12/20], Loss: 0.001620, Time: 69.92 sec\n",
      "Epoch [13/20], Loss: 0.001575, Time: 67.42 sec\n",
      "Epoch [14/20], Loss: 0.001548, Time: 67.45 sec\n",
      "Epoch [15/20], Loss: 0.001920, Time: 67.45 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [16/20], Loss: 0.010073, Time: 69.39 sec\n",
      "No improvement. Patience: 2/2\n",
      "Early stopping triggered.\n",
      "Loaded best model with lowest validation loss.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "model.train()  \n",
    "best_loss = float('inf')\n",
    "patience = 2  \n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)  \n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, data)\n",
    "        epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()     \n",
    "        optimizer.step()       \n",
    "\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}, Time: {elapsed:.2f} sec\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss - 1e-6:  \n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict() \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PSNR: 20.25 dB\n",
      "Test SSIM: 0.6026\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageFolderNoClass(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [os.path.join(folder_path, f) \n",
    "                           for f in os.listdir(folder_path) \n",
    "                           if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolderNoClass('./BSD500/train', transform=transform)\n",
    "val_dataset   = ImageFolderNoClass('./BSD500/val', transform=transform)\n",
    "test_dataset  = ImageFolderNoClass('./BSD500/test', transform=transform)\n",
    "\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset+val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(train_dataset+val_dataset+test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        \n",
    "        # Move tensors to CPU and convert to numpy arrays, clipping values into [0,1]\n",
    "        output_np = output.cpu().numpy().transpose(0, 2, 3, 1)   # (N, H, W, C)\n",
    "        clean_np  = data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        noisy_np  = noisy_data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Calculate metrics image by image\n",
    "        for denoised, clean in zip(output_np, clean_np):\n",
    "            denoised = np.clip(denoised, 0., 1.)\n",
    "            clean = np.clip(clean, 0., 1.)\n",
    "            psnr_val = calculate_psnr(denoised, clean)\n",
    "            ssim_val = calculate_ssim(denoised, clean)\n",
    "            psnr_list.append(psnr_val)\n",
    "            ssim_list.append(ssim_val)\n",
    "\n",
    "mean_psnr = np.mean(psnr_list)\n",
    "mean_ssim = np.mean(ssim_list)\n",
    "\n",
    "print(f\"Test PSNR: {mean_psnr:.2f} dB\")\n",
    "print(f\"Test SSIM: {mean_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
