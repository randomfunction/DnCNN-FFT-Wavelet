{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting Training with Early Stopping...\n",
      "Epoch [1/100], Loss: 0.007151, Time: 33.77 sec\n",
      "Epoch [2/100], Loss: 0.002474, Time: 32.64 sec\n",
      "Epoch [3/100], Loss: 0.002279, Time: 32.61 sec\n",
      "Epoch [4/100], Loss: 0.002157, Time: 32.63 sec\n",
      "Epoch [5/100], Loss: 0.002050, Time: 32.52 sec\n",
      "Epoch [6/100], Loss: 0.001981, Time: 32.43 sec\n",
      "Epoch [7/100], Loss: 0.001924, Time: 32.44 sec\n",
      "Epoch [8/100], Loss: 0.001867, Time: 32.42 sec\n",
      "Epoch [9/100], Loss: 0.001821, Time: 32.46 sec\n",
      "Epoch [10/100], Loss: 0.001787, Time: 32.43 sec\n",
      "Epoch [11/100], Loss: 0.001759, Time: 32.43 sec\n",
      "Epoch [12/100], Loss: 0.001744, Time: 32.49 sec\n",
      "Epoch [13/100], Loss: 0.001722, Time: 32.53 sec\n",
      "Epoch [14/100], Loss: 0.001710, Time: 32.67 sec\n",
      "Epoch [15/100], Loss: 0.001697, Time: 32.62 sec\n",
      "Epoch [16/100], Loss: 0.001682, Time: 32.79 sec\n",
      "Epoch [17/100], Loss: 0.001673, Time: 32.68 sec\n",
      "Epoch [18/100], Loss: 0.001663, Time: 32.88 sec\n",
      "Epoch [19/100], Loss: 0.001651, Time: 32.66 sec\n",
      "Epoch [20/100], Loss: 0.001645, Time: 32.77 sec\n",
      "Epoch [21/100], Loss: 0.001636, Time: 32.65 sec\n",
      "Epoch [22/100], Loss: 0.001630, Time: 32.71 sec\n",
      "Epoch [23/100], Loss: 0.001623, Time: 34.22 sec\n",
      "Epoch [24/100], Loss: 0.001616, Time: 34.93 sec\n",
      "Epoch [25/100], Loss: 0.001612, Time: 35.33 sec\n",
      "Epoch [26/100], Loss: 0.001607, Time: 32.43 sec\n",
      "Epoch [27/100], Loss: 0.001602, Time: 32.56 sec\n",
      "Epoch [28/100], Loss: 0.001599, Time: 36.87 sec\n",
      "Epoch [29/100], Loss: 0.001595, Time: 35.79 sec\n",
      "Epoch [30/100], Loss: 0.001591, Time: 35.58 sec\n",
      "Epoch [31/100], Loss: 0.001588, Time: 35.97 sec\n",
      "Epoch [32/100], Loss: 0.001585, Time: 36.42 sec\n",
      "Epoch [33/100], Loss: 0.001582, Time: 36.10 sec\n",
      "Epoch [34/100], Loss: 0.001581, Time: 32.72 sec\n",
      "Epoch [35/100], Loss: 0.001576, Time: 32.01 sec\n",
      "Epoch [36/100], Loss: 0.001576, Time: 31.98 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [37/100], Loss: 0.001572, Time: 31.98 sec\n",
      "Epoch [38/100], Loss: 0.001570, Time: 31.98 sec\n",
      "Epoch [39/100], Loss: 0.001568, Time: 31.95 sec\n",
      "Epoch [40/100], Loss: 0.001565, Time: 31.93 sec\n",
      "Epoch [41/100], Loss: 0.001563, Time: 32.05 sec\n",
      "Epoch [42/100], Loss: 0.001561, Time: 31.91 sec\n",
      "Epoch [43/100], Loss: 0.001561, Time: 31.93 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [44/100], Loss: 0.001558, Time: 32.42 sec\n",
      "Epoch [45/100], Loss: 0.001556, Time: 32.28 sec\n",
      "Epoch [46/100], Loss: 0.001555, Time: 32.10 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [47/100], Loss: 0.001554, Time: 32.12 sec\n",
      "Epoch [48/100], Loss: 0.001552, Time: 32.11 sec\n",
      "Epoch [49/100], Loss: 0.001551, Time: 32.09 sec\n",
      "Epoch [50/100], Loss: 0.001551, Time: 32.10 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [51/100], Loss: 0.001549, Time: 32.10 sec\n",
      "Epoch [52/100], Loss: 0.001548, Time: 32.11 sec\n",
      "Epoch [53/100], Loss: 0.001547, Time: 32.09 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [54/100], Loss: 0.001545, Time: 32.12 sec\n",
      "Epoch [55/100], Loss: 0.001545, Time: 32.10 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [56/100], Loss: 0.001544, Time: 32.07 sec\n",
      "Epoch [57/100], Loss: 0.001542, Time: 32.11 sec\n",
      "Epoch [58/100], Loss: 0.001542, Time: 32.01 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [59/100], Loss: 0.001543, Time: 32.00 sec\n",
      "No improvement. Patience: 2/2\n",
      "Early stopping triggered.\n",
      "Loaded best model with lowest validation loss.\n",
      "Loaded best model with lowest validation loss.\n",
      "Best model saved as 'best_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "import torch                               \n",
    "import torch.nn as nn                      \n",
    "import torch.optim as optim                \n",
    "from torch.utils.data import DataLoader   \n",
    "import torchvision                  \n",
    "import torchvision.transforms as transforms  \n",
    "import numpy as np                       \n",
    "import cv2                                \n",
    "from skimage.metrics import structural_similarity as ssim  \n",
    "import matplotlib.pyplot as plt         \n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def calculate_psnr(denoised, ground_truth):\n",
    "    mse = np.mean((denoised - ground_truth) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    PIXEL_MAX = 1.0 \n",
    "    psnr = 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "    return psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def calculate_ssim(denoised, ground_truth):\n",
    "    return ssim(ground_truth, denoised, data_range=ground_truth.max() - ground_truth.min(), win_size=7, channel_axis=-1)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.fft import fft2, ifft2\n",
    "\n",
    "# Define a single FNet Block that performs FFT-based global feature mixing\n",
    "class FNetBlock(nn.Module):\n",
    "    def __init__(self, channels, height, width):\n",
    "        super(FNetBlock, self).__init__()\n",
    "        \n",
    "        # LayerNorm applied over full [C, H, W] dimensions\n",
    "        self.layer_norm = nn.LayerNorm([channels, height, width])\n",
    "        \n",
    "        # Feedforward network with 1x1 convolutions for feature projection\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),  # Project + mix channel-wise features\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1)   # Final refinement\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply 2D FFT to each channel to capture global frequency features\n",
    "        x_fft = torch.fft.fft2(x, dim=(-2, -1))\n",
    "        \n",
    "        # Convert back to spatial domain (real part only)\n",
    "        x_ifft = torch.fft.ifft2(x_fft, dim=(-2, -1)).real\n",
    "        \n",
    "        # Normalize the result to stabilize feature values\n",
    "        x_norm = self.layer_norm(x_ifft)\n",
    "        \n",
    "        # Pass through FFN and add residual connection (like ResNet)\n",
    "        x_out = x_norm + self.ffn(x_norm)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "\n",
    "# Full DnCNN-based denoising model with FNet Blocks replacing standard convolutions\n",
    "class FnetDnCNNResidual(nn.Module):\n",
    "    def __init__(self, channels=3, num_features=64, num_fnet_blocks=10, height=32, width=32):\n",
    "        super(FnetDnCNNResidual, self).__init__()\n",
    "\n",
    "        # Head: basic conv layer to extract initial features from input\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(channels, num_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # FNetBlocks stack: performs global mixing and normalization\n",
    "        self.fnet_blocks = nn.Sequential(\n",
    "            *[FNetBlock(num_features, height, width) for _ in range(num_fnet_blocks)]\n",
    "        )\n",
    "\n",
    "        # Tail: reduce feature maps back to original channel dimension (for noise prediction)\n",
    "        self.tail = nn.Conv2d(num_features, channels, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from input image\n",
    "        features = self.head(x)\n",
    "\n",
    "        # Pass features through FNet-based global attention blocks\n",
    "        features = self.fnet_blocks(features)\n",
    "\n",
    "        # Predict the residual noise in the image\n",
    "        predicted_noise = self.tail(features)\n",
    "    \n",
    "        # Subtract predicted noise from input to get denoised output\n",
    "        denoised = x - predicted_noise\n",
    "\n",
    "        return denoised\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model = DnCNN_FFT(channels=3).to(device)\n",
    "model = FnetDnCNNResidual(channels=3, num_features=64, num_fnet_blocks=3, height=32, width=32).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "noise_std = 0.1  \n",
    "\n",
    "print(\"Starting Training with Early Stopping...\")\n",
    "model.train()\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 2  \n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, data)\n",
    "        epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}, Time: {elapsed:.2f} sec\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss - 1e-6:  \n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict() \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n",
    "\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n",
    "    torch.save(model.state_dict(), 'fft_model.pth')\n",
    "    print(\"Best model saved as 'best_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PSNR: 29.04 dB\n",
      "Test SSIM: 0.8924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageFolderNoClass(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [os.path.join(folder_path, f) \n",
    "                           for f in os.listdir(folder_path) \n",
    "                           if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# train_dataset = ImageFolderNoClass('./BSD500/train', transform=transform)\n",
    "# val_dataset   = ImageFolderNoClass('./BSD500/val', transform=transform)\n",
    "test_dataset  = ImageFolderNoClass('./BSD68/BSD68', transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset+val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        \n",
    "        # Move tensors to CPU and convert to numpy arrays, clipping values into [0,1]\n",
    "        output_np = output.cpu().numpy().transpose(0, 2, 3, 1)   # (N, H, W, C)\n",
    "        clean_np  = data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        noisy_np  = noisy_data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Calculate metrics image by image\n",
    "        for denoised, clean in zip(output_np, clean_np):\n",
    "            denoised = np.clip(denoised, 0., 1.)\n",
    "            clean = np.clip(clean, 0., 1.)\n",
    "            psnr_val = calculate_psnr(denoised, clean)\n",
    "            ssim_val = calculate_ssim(denoised, clean)\n",
    "            psnr_list.append(psnr_val)\n",
    "            ssim_list.append(ssim_val)\n",
    "\n",
    "mean_psnr = np.mean(psnr_list)\n",
    "mean_ssim = np.mean(ssim_list)\n",
    "\n",
    "print(f\"Test PSNR: {mean_psnr:.2f} dB\")\n",
    "print(f\"Test SSIM: {mean_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/100], Loss: 0.009476, Time: 72.08 sec\n",
      "Epoch [2/100], Loss: 0.003776, Time: 70.72 sec\n",
      "Epoch [3/100], Loss: 0.002577, Time: 70.76 sec\n",
      "Epoch [4/100], Loss: 0.002205, Time: 70.74 sec\n",
      "Epoch [5/100], Loss: 0.001924, Time: 70.65 sec\n",
      "Epoch [6/100], Loss: 0.001778, Time: 70.68 sec\n",
      "Epoch [7/100], Loss: 0.001698, Time: 70.79 sec\n",
      "Epoch [8/100], Loss: 0.001671, Time: 70.69 sec\n",
      "Epoch [9/100], Loss: 0.001619, Time: 70.70 sec\n",
      "Epoch [10/100], Loss: 0.001599, Time: 70.74 sec\n",
      "Epoch [11/100], Loss: 0.001570, Time: 70.68 sec\n",
      "Epoch [12/100], Loss: 0.001560, Time: 70.62 sec\n",
      "Epoch [13/100], Loss: 0.001603, Time: 70.75 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [14/100], Loss: 0.001537, Time: 70.76 sec\n",
      "Epoch [15/100], Loss: 0.001518, Time: 70.66 sec\n",
      "Epoch [16/100], Loss: 0.001522, Time: 70.74 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [17/100], Loss: 0.001507, Time: 70.76 sec\n",
      "Epoch [18/100], Loss: 0.001536, Time: 70.74 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [19/100], Loss: 0.001518, Time: 70.67 sec\n",
      "No improvement. Patience: 2/2\n",
      "Early stopping triggered.\n",
      "Loaded best model with lowest validation loss.\n",
      "Loaded best model with lowest validation loss.\n",
      "Best model saved as 'best_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_wavelets import DWT, IDWT\n",
    "\n",
    "class WaveletBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts the noise residual in the wavelet domain.\n",
    "    Uses DWT (Discrete Wavelet Transform) to analyze image in frequency + spatial domain.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, wavelet='haar'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Forward and inverse wavelet transforms (1-level decomposition)\n",
    "        self.dwt = DWT(J=1, wave=wavelet)\n",
    "        self.idwt = IDWT(wave=wavelet)\n",
    "\n",
    "        # Feed-forward network to process wavelet features (all 4 subbands)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(4 * channels, 4 * channels, kernel_size=1, padding=0),  # Project + mix across channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(4 * channels, 4 * channels, kernel_size=1, padding=0)   # Final projection\n",
    "        )\n",
    "\n",
    "        # Learnable soft-thresholding parameters for LH, HL, HH bands\n",
    "        self.threshold = nn.Parameter(torch.zeros(3, channels, 1, 1))  # Shape: [3 subbands, C, 1, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Discrete Wavelet Transform\n",
    "        ll, yh = self.dwt(x)           # ll: low-frequency approx, yh: list of high-freq details\n",
    "\n",
    "        detail = yh[0]                 # Extract the first-level detail subbands\n",
    "        lh, hl, hh = torch.unbind(detail, dim=2)  # Separate into LH, HL, HH components\n",
    "\n",
    "        # Concatenate all 4 bands (LL, LH, HL, HH) for processing\n",
    "        stacked = torch.cat([ll, lh, hl, hh], dim=1)\n",
    "\n",
    "        # Pass through feedforward projection network\n",
    "        y = self.ffn(stacked)\n",
    "\n",
    "        c = x.size(1)  # Number of channels\n",
    "\n",
    "        # Split the processed features back into subbands\n",
    "        ll2, lh2, hl2, hh2 = torch.split(y, c, dim=1)\n",
    "\n",
    "        # Apply learnable soft-thresholding to high-frequency bands\n",
    "        t = torch.sigmoid(self.threshold)  # Thresholds in [0, 1] range\n",
    "        lh2 = lh2 * t[0]\n",
    "        hl2 = hl2 * t[1]\n",
    "        hh2 = hh2 * t[2]\n",
    "\n",
    "        # Stack high-frequency bands back into required shape for IDWT: [B, C, 3, H/2, W/2]\n",
    "        y_high = torch.stack([lh2, hl2, hh2], dim=2)\n",
    "\n",
    "        # Apply inverse DWT to get back to spatial domain (predicted residual noise)\n",
    "        out = self.idwt((ll2, [y_high]))\n",
    "        return out  # Final residual noise prediction from wavelet block\n",
    "\n",
    "\n",
    "class HybridDnCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines standard DnCNN residual prediction with a wavelet-based residual branch.\n",
    "    Input: noisy image x\n",
    "    Output: denoised image\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=3, num_layers=17, features=64, wavelet='haar'):\n",
    "        super(HybridDnCNN, self).__init__()\n",
    "        # Standard DnCNN branch\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=3, padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers += [\n",
    "                nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "        layers.append(nn.Conv2d(features, channels, kernel_size=3, padding=1, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "        # Wavelet residual branch\n",
    "        self.wavelet_block = WaveletBlock(channels, wavelet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise_dn = self.dncnn(x)\n",
    "        noise_wave = self.wavelet_block(x)\n",
    "        noise_combined = noise_dn + noise_wave\n",
    "        clean = x - noise_combined\n",
    "        return clean\n",
    "    \n",
    "\n",
    "model = HybridDnCNN(channels=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "noise_std = 0.1 \n",
    "\n",
    "print(\"Starting Training...\")\n",
    "model.train()  \n",
    "best_loss = float('inf')\n",
    "patience = 2  \n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data,_ in train_loader:\n",
    "        data = data.to(device)  \n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, data)\n",
    "        epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()     \n",
    "        optimizer.step()       \n",
    "\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}, Time: {elapsed:.2f} sec\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss - 1e-6:  \n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict() \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n",
    "\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n",
    "    torch.save(model.state_dict(), 'wavelet_model.pth')\n",
    "    print(\"Best model saved as 'best_model.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PSNR: 29.05 dB\n",
      "Test SSIM: 0.9008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageFolderNoClass(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [os.path.join(folder_path, f) \n",
    "                           for f in os.listdir(folder_path) \n",
    "                           if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# train_dataset = ImageFolderNoClass('./BSD500/train', transform=transform)\n",
    "# val_dataset   = ImageFolderNoClass('./BSD500/val', transform=transform)\n",
    "test_dataset  = ImageFolderNoClass('./BSD68/BSD68', transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset+val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        \n",
    "        # Move tensors to CPU and convert to numpy arrays, clipping values into [0,1]\n",
    "        output_np = output.cpu().numpy().transpose(0, 2, 3, 1)   # (N, H, W, C)\n",
    "        clean_np  = data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        noisy_np  = noisy_data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Calculate metrics image by image\n",
    "        for denoised, clean in zip(output_np, clean_np):\n",
    "            denoised = np.clip(denoised, 0., 1.)\n",
    "            clean = np.clip(clean, 0., 1.)\n",
    "            psnr_val = calculate_psnr(denoised, clean)\n",
    "            ssim_val = calculate_ssim(denoised, clean)\n",
    "            psnr_list.append(psnr_val)\n",
    "            ssim_list.append(ssim_val)\n",
    "\n",
    "mean_psnr = np.mean(psnr_list)\n",
    "mean_ssim = np.mean(ssim_list)\n",
    "\n",
    "print(f\"Test PSNR: {mean_psnr:.2f} dB\")\n",
    "print(f\"Test SSIM: {mean_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
