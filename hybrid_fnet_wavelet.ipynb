{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                              \n",
    "import torch.nn as nn                      \n",
    "import torch.optim as optim                \n",
    "from torch.utils.data import DataLoader    \n",
    "import torchvision                       \n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np                         \n",
    "import cv2                                \n",
    "from skimage.metrics import structural_similarity as ssim  \n",
    "import matplotlib.pyplot as plt           \n",
    "import time\n",
    "from pytorch_wavelets import DWT, IDWT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(denoised, ground_truth):\n",
    "    mse = np.mean((denoised - ground_truth) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    PIXEL_MAX = 1.0 \n",
    "    psnr = 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "    return psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def calculate_ssim(denoised, ground_truth):\n",
    "    return ssim(ground_truth, denoised, data_range=ground_truth.max() - ground_truth.min(), win_size=7, channel_axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize((64,64)) #update\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 training and test datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.fft import fft2, ifft2\n",
    "from pytorch_wavelets import DWT, IDWT\n",
    "\n",
    "class FNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial-frequency mixing via FFT, with residual feed-forward.\n",
    "    This block mixes information in both spatial and frequency domains.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, height, width):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm([channels, height, width])\n",
    "        # Feed-forward network (FFN) consisting of two Conv2d layers with a ReLU in between\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),  # 1x1 convolution to adjust channels\n",
    "            nn.ReLU(inplace=True),  # ReLU activation for non-linearity\n",
    "            nn.Conv2d(channels, channels, kernel_size=1)   # Another 1x1 convolution\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply FFT and IFFT to the input tensor (x), working in frequency space\n",
    "        x_fft = fft2(x, dim=(-2, -1))  # Fast Fourier Transform (FFT)\n",
    "        x_ifft = ifft2(x_fft, dim=(-2, -1)).real  # Inverse FFT (IFFT), only keeping real part\n",
    "        x_norm = self.layer_norm(x_ifft)  # Normalize the result in the spatial domain\n",
    "        # Add residual connection with feed-forward network applied on the normalized result\n",
    "        return x_ifft + self.ffn(x_norm)\n",
    "\n",
    "\n",
    "class WaveletBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts residual noise in the wavelet domain using single-level DWT.\n",
    "    This block uses Discrete Wavelet Transform (DWT) to decompose the input, \n",
    "    processes the details, and then reconstructs the residual noise.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, wavelet='haar'):\n",
    "        super().__init__()\n",
    "        self.dwt = DWT(J=1, wave=wavelet)  # Single-level DWT with specified wavelet\n",
    "        self.idwt = IDWT(wave=wavelet)  # Inverse DWT for reconstruction\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(4 * channels, 4 * channels, kernel_size=1, padding=0),  # 1x1 convolution to adjust channels\n",
    "            nn.ReLU(inplace=True),  # ReLU activation for non-linearity\n",
    "            nn.Conv2d(4 * channels, 4 * channels, kernel_size=1, padding=0),  # Another 1x1 convolution\n",
    "        )\n",
    "        self.threshold = nn.Parameter(torch.zeros(3, channels, 1, 1))  # Threshold to scale the wavelet details\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decompose the input tensor into wavelet sub-bands (low and high frequency components)\n",
    "        ll, yh = self.dwt(x)\n",
    "        detail = yh[0]  # High-frequency components (details)\n",
    "        lh, hl, hh = torch.unbind(detail, dim=2)  # Split high-frequency details into 3 sub-bands\n",
    "        # Concatenate low and high-frequency components along the channel dimension\n",
    "        stacked = torch.cat([ll, lh, hl, hh], dim=1)\n",
    "        # Apply feed-forward network to the stacked components\n",
    "        y = self.ffn(stacked)\n",
    "        # Split the output back into corresponding sub-bands\n",
    "        c = x.size(1)\n",
    "        ll2, lh2, hl2, hh2 = torch.split(y, c, dim=1)\n",
    "        t = torch.sigmoid(self.threshold)  # Sigmoid to generate scaling factors for the sub-bands\n",
    "        lh2 = lh2 * t[0]  # Scale the lh sub-band\n",
    "        hl2 = hl2 * t[1]  # Scale the hl sub-band\n",
    "        hh2 = hh2 * t[2]  # Scale the hh sub-band\n",
    "        y_high = torch.stack([lh2, hl2, hh2], dim=2)  # Stack the scaled high-frequency sub-bands\n",
    "        # Reconstruct the residual noise using the inverse DWT\n",
    "        out = self.idwt((ll2, [y_high]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class DnCNNBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard DnCNN residual noise predictor.\n",
    "    This is a simple deep neural network that predicts the noise residuals from the input image.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=3, num_layers=17, features=64):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(channels, features, kernel_size=3, padding=1, bias=True),  # First layer\n",
    "                  nn.ReLU(inplace=True)]  # ReLU activation\n",
    "        # Add several convolutional layers followed by batch normalization and ReLU\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers += [nn.Conv2d(features, features, 3, padding=1, bias=False),\n",
    "                       nn.BatchNorm2d(features),\n",
    "                       nn.ReLU(inplace=True)]\n",
    "        # Final convolutional layer to match output channels with input\n",
    "        layers.append(nn.Conv2d(features, channels, 3, padding=1, bias=False))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the DnCNN network\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class HybridTriBranchModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid denoiser combining DnCNN, FNet, and Wavelet residual branches.\n",
    "    This model uses three separate branches (DnCNN, FNet, and Wavelet) to predict residual noise\n",
    "    and then fuses the results to denoise the input image.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 channels=3,\n",
    "                 height=32,\n",
    "                 width=32,\n",
    "                 dncnn_layers=17,\n",
    "                 dncnn_features=64,\n",
    "                 fnet_blocks=5,\n",
    "                 wavelet='haar'):\n",
    "        super().__init__()\n",
    "        # Define the branches: DnCNN, FNet, and Wavelet\n",
    "        self.dncnn_branch = DnCNNBranch(channels, dncnn_layers, dncnn_features)\n",
    "        self.fnet_blocks = nn.Sequential(\n",
    "            *[FNetBlock(dncnn_features, height, width) for _ in range(fnet_blocks)]  # Stack FNet blocks\n",
    "        )\n",
    "        self.wavelet_branch = WaveletBlock(channels, wavelet)\n",
    "        # Fusion layer: combines the residual predictions from the three branches\n",
    "        self.fusion = nn.Conv2d(channels * 3, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Predict residual noise using each branch\n",
    "        r_dn = self.dncnn_branch(x)\n",
    "        # Project input through DnCNN to obtain features for FNet\n",
    "        feat = self.dncnn_branch.net[0:2](x)  # First two layers of DnCNN (Conv + ReLU)\n",
    "        feat = self.fnet_blocks(feat)  # Apply FNet blocks\n",
    "        r_fnet = self.dncnn_branch.net[-1:](feat)  # Final layer to bring it back to input channels\n",
    "        r_wave = self.wavelet_branch(x)  # Predict residual from wavelet branch\n",
    "        # Concatenate the residuals from all three branches\n",
    "        r_cat = torch.cat([r_dn, r_fnet, r_wave], dim=1)\n",
    "        # Fuse the concatenated residuals and subtract from the input for denoising\n",
    "        r = self.fusion(r_cat)\n",
    "        return x - r  # Denoised output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridTriBranchModel(channels=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50\n",
    "noise_std = 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/50], Loss: 0.003217, Time: 125.73 sec\n",
      "Epoch [2/50], Loss: 0.001703, Time: 125.88 sec\n",
      "Epoch [3/50], Loss: 0.001580, Time: 126.47 sec\n",
      "Epoch [4/50], Loss: 0.001530, Time: 126.38 sec\n",
      "Epoch [5/50], Loss: 0.001500, Time: 125.12 sec\n",
      "Epoch [6/50], Loss: 0.001480, Time: 126.84 sec\n",
      "Epoch [7/50], Loss: 0.001469, Time: 124.52 sec\n",
      "Epoch [8/50], Loss: 0.001458, Time: 124.38 sec\n",
      "Epoch [9/50], Loss: 0.001451, Time: 126.03 sec\n",
      "Epoch [10/50], Loss: 0.001439, Time: 125.47 sec\n",
      "Epoch [11/50], Loss: 0.001413, Time: 125.74 sec\n",
      "Epoch [12/50], Loss: 0.001374, Time: 125.23 sec\n",
      "Epoch [13/50], Loss: 0.001354, Time: 125.92 sec\n",
      "Epoch [14/50], Loss: 0.001343, Time: 125.79 sec\n",
      "Epoch [15/50], Loss: 0.001333, Time: 125.52 sec\n",
      "Epoch [16/50], Loss: 0.001326, Time: 125.10 sec\n",
      "Epoch [17/50], Loss: 0.001321, Time: 125.35 sec\n",
      "Epoch [18/50], Loss: 0.001316, Time: 125.13 sec\n",
      "Epoch [19/50], Loss: 0.001312, Time: 125.44 sec\n",
      "Epoch [20/50], Loss: 0.001309, Time: 125.33 sec\n",
      "Epoch [21/50], Loss: 0.001307, Time: 125.04 sec\n",
      "Epoch [22/50], Loss: 0.001303, Time: 124.58 sec\n",
      "Epoch [23/50], Loss: 0.001301, Time: 125.62 sec\n",
      "Epoch [24/50], Loss: 0.001299, Time: 126.08 sec\n",
      "Epoch [25/50], Loss: 0.001296, Time: 121.55 sec\n",
      "Epoch [26/50], Loss: 0.001295, Time: 124.23 sec\n",
      "Epoch [27/50], Loss: 0.001294, Time: 124.18 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [28/50], Loss: 0.001292, Time: 124.95 sec\n",
      "Epoch [29/50], Loss: 0.001290, Time: 124.96 sec\n",
      "Epoch [30/50], Loss: 0.001288, Time: 124.66 sec\n",
      "Epoch [31/50], Loss: 0.001287, Time: 123.06 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [32/50], Loss: 0.001286, Time: 125.01 sec\n",
      "Epoch [33/50], Loss: 0.001285, Time: 124.43 sec\n",
      "Epoch [34/50], Loss: 0.001283, Time: 124.90 sec\n",
      "Epoch [35/50], Loss: 0.001283, Time: 125.13 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [36/50], Loss: 0.001282, Time: 125.84 sec\n",
      "Epoch [37/50], Loss: 0.001280, Time: 123.72 sec\n",
      "Epoch [38/50], Loss: 0.001279, Time: 123.05 sec\n",
      "Epoch [39/50], Loss: 0.001278, Time: 123.04 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [40/50], Loss: 0.001276, Time: 123.66 sec\n",
      "Epoch [41/50], Loss: 0.001276, Time: 123.00 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [42/50], Loss: 0.001275, Time: 123.53 sec\n",
      "Epoch [43/50], Loss: 0.001274, Time: 124.28 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [44/50], Loss: 0.001274, Time: 124.28 sec\n",
      "Epoch [45/50], Loss: 0.001274, Time: 124.21 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [46/50], Loss: 0.001272, Time: 123.61 sec\n",
      "Epoch [47/50], Loss: 0.001271, Time: 123.50 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [48/50], Loss: 0.001271, Time: 123.74 sec\n",
      "Epoch [49/50], Loss: 0.001270, Time: 123.79 sec\n",
      "No improvement. Patience: 1/2\n",
      "Epoch [50/50], Loss: 0.001269, Time: 123.70 sec\n",
      "Loaded best model with lowest validation loss.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "model.train()  \n",
    "best_loss = float('inf')\n",
    "patience = 2  \n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)  \n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, data)\n",
    "        epoch_loss += loss.item() * data.size(0)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()     \n",
    "        optimizer.step()       \n",
    "\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}, Time: {elapsed:.2f} sec\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_loss < best_loss - 1e-6:  \n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict() \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model with lowest validation loss.\n",
      "Best model saved as 'best_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model with lowest validation loss.\")\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(\"Best model saved as 'best_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PSNR: 29.77 dB\n",
      "Test SSIM: 0.9104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageFolderNoClass(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [os.path.join(folder_path, f) \n",
    "                           for f in os.listdir(folder_path) \n",
    "                           if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# train_dataset = ImageFolderNoClass('./BSD500/train', transform=transform)\n",
    "# val_dataset   = ImageFolderNoClass('./BSD500/val', transform=transform)\n",
    "test_dataset  = ImageFolderNoClass('./BSD68/BSD68', transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset+val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        noise = torch.randn_like(data) * noise_std\n",
    "        noisy_data = data + noise\n",
    "        output = model(noisy_data)\n",
    "        \n",
    "        # Move tensors to CPU and convert to numpy arrays, clipping values into [0,1]\n",
    "        output_np = output.cpu().numpy().transpose(0, 2, 3, 1)   # (N, H, W, C)\n",
    "        clean_np  = data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        noisy_np  = noisy_data.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        \n",
    "        # Calculate metrics image by image\n",
    "        for denoised, clean in zip(output_np, clean_np):\n",
    "            denoised = np.clip(denoised, 0., 1.)\n",
    "            clean = np.clip(clean, 0., 1.)\n",
    "            psnr_val = calculate_psnr(denoised, clean)\n",
    "            ssim_val = calculate_ssim(denoised, clean)\n",
    "            psnr_list.append(psnr_val)\n",
    "            ssim_list.append(ssim_val)\n",
    "\n",
    "mean_psnr = np.mean(psnr_list)\n",
    "mean_ssim = np.mean(ssim_list)\n",
    "\n",
    "print(f\"Test PSNR: {mean_psnr:.2f} dB\")\n",
    "print(f\"Test SSIM: {mean_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
